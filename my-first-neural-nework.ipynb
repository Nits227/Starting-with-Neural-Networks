{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"},{"sourceId":13122830,"sourceType":"datasetVersion","datasetId":8312853}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom tqdm import tqdm\nimport os\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nclass DigitClassifier(nn.Module):\n  \n \n    def __init__(self, dropout_rate=0.2):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 256)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout_rate)  # Prevent overfitting\n        self.fc2 = nn.Linear(256, 128)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout_rate)  # Prevent overfitting\n        self.fc3 = nn.Linear(128, 10)  # No dropout on output layer\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.flatten(x)\n        x = self.relu1(self.fc1(x))\n        x = self.dropout1(x)  # Applied during training only\n        x = self.relu2(self.fc2(x))\n        x = self.dropout2(x)  # Applied during training only\n        x = self.fc3(x)\n        return x  # logits (not probabilities)\n\nclass CNNDigitClassifier(nn.Module):\n  \n    def __init__(self, dropout_rate=0.25):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2)\n        self.dropout1 = nn.Dropout2d(0.25)  # 2D dropout for conv layers\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.dropout2 = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(128, 10)\n        \n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.dropout1(x)\n        x = x.view(-1, 64 * 7 * 7)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\ndef load_data_with_validation(batch_size: int, validation_split=0.1):\n  \n    # Consistent base transform for test data\n    base_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))  # MNIST standard values\n    ])\n    \n    # Enhanced augmentation for training data\n    train_transform = transforms.Compose([\n        transforms.RandomRotation(10),\n        transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))  # Same normalization as test\n    ])\n    \n    # Load datasets\n    full_train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n    test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=base_transform)\n    \n    # Split training data into train and validation\n    train_size = int((1 - validation_split) * len(full_train_dataset))\n    val_size = len(full_train_dataset) - train_size\n    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n    \n    print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n    return train_loader, val_loader, test_loader\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, max_epochs=50):\n    \n    model.to(device)\n    \n    best_val_acc = 0\n    patience_counter = 0\n    patience = 5  # Stop if no improvement for 5 epochs\n    \n    print(f\"Starting training for up to {max_epochs} epochs...\")\n    print(\"-\" * 60)\n    \n    for epoch in range(max_epochs):\n        # Training phase\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1:2d}/{max_epochs}\", ncols=100)\n        for imgs, labels in train_pbar:\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, preds = torch.max(logits, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            \n            # Update progress bar\n            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n\n        # Calculate training metrics\n        avg_loss = total_loss / len(train_loader)\n        train_accuracy = 100. * correct / total\n        \n        # Validation phase\n        val_accuracy = evaluate_model(model, val_loader, return_accuracy=True, verbose=False)\n        \n        print(f\"Epoch {epoch+1:2d}: Train Loss: {avg_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n        \n        # Learning rate scheduling\n        if scheduler:\n            scheduler.step()\n            current_lr = scheduler.get_last_lr()[0]\n            if epoch > 0:  # Don't print LR on first epoch\n                print(f\"         Learning rate: {current_lr:.6f}\")\n        \n        # Early stopping logic\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            patience_counter = 0\n            # Save best model\n            torch.save(model.state_dict(), 'best_mnist_model.pth')\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping at epoch {epoch+1}!\")\n            print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n            break\n    \n    # Load best model weights\n    model.load_state_dict(torch.load('best_mnist_model.pth'))\n    print(f\"\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%\")\n    return best_val_acc\n\ndef evaluate_model(model, test_loader, return_accuracy=False, verbose=True):\n    \"\"\"\n    Evaluate the model on the test dataset.\n    \"\"\"\n    model.eval()\n    model.to(device)\n    \n    correct, total = 0, 0\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            logits = model(imgs)\n            _, preds = torch.max(logits, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = 100. * correct / total\n    if verbose:\n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n    \n    if return_accuracy:\n        return accuracy\n\ndef save_model(model, filepath, metadata=None):\n    \"\"\"\n    Save model with metadata.\n    \"\"\"\n    save_dict = {\n        'model_state_dict': model.state_dict(),\n        'model_class': model.__class__.__name__\n    }\n    if metadata:\n        save_dict.update(metadata)\n    \n    torch.save(save_dict, filepath)\n    print(f\"Model saved to {filepath}\")\n\ndef load_model(filepath, model_class):\n    \"\"\"\n    Load saved model.\n    \"\"\"\n    checkpoint = torch.load(filepath)\n    model = model_class()\n    model.load_state_dict(checkpoint['model_state_dict'])\n    return model\n\ndef count_parameters(model):\n    \"\"\"\n    Count trainable parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nif __name__ == \"__main__\":\n    # Hyperparameters\n    batch_size = 64\n    learning_rate = 0.001\n    max_epochs = 50\n    dropout_rate = 0.2\n    use_cnn = False  # Set to True to use CNN instead of fully connected\n    \n    print(\"=\" * 60)\n    print(\"MNIST Digit Classification - Improved Version\")\n    print(\"=\" * 60)\n    \n    # Load data with validation split\n    train_loader, val_loader, test_loader = load_data_with_validation(batch_size)\n\n    # Initialize model\n    if use_cnn:\n        model = CNNDigitClassifier(dropout_rate=dropout_rate)\n        print(\"Using CNN architecture\")\n    else:\n        model = DigitClassifier(dropout_rate=dropout_rate)\n        print(\"Using fully connected architecture\")\n    \n    print(f\"Model has {count_parameters(model):,} trainable parameters\")\n    \n    # Loss function and optimizer with L2 regularization\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    \n    print(f\"Training configuration:\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Learning rate: {learning_rate}\")\n    print(f\"  Dropout rate: {dropout_rate}\")\n    print(f\"  Max epochs: {max_epochs}\")\n    print(f\"  Weight decay: 1e-4\")\n    \n    # Train the model\n    best_val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, max_epochs)\n\n    # Final evaluation on test set\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FINAL RESULTS\")\n    print(\"=\" * 60)\n    test_acc = evaluate_model(model, test_loader)\n    \n    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n    print(f\"Final test accuracy: {test_acc:.2f}%\")\n    \n    # Save the final model\n    metadata = {\n        'test_accuracy': test_acc,\n        'best_val_accuracy': best_val_acc,\n        'hyperparameters': {\n            'batch_size': batch_size,\n            'learning_rate': learning_rate,\n            'dropout_rate': dropout_rate,\n            'architecture': 'CNN' if use_cnn else 'FC'\n        }\n    }\n    \n    save_model(model, 'final_mnist_model.pth', metadata)\n    \n    # Clean up temporary best model file\n    if os.path.exists('best_mnist_model.pth'):\n        os.remove('best_mnist_model.pth')\n    \n    print(\"\\nTraining completed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
